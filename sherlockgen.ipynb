{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Import Clean Data\n",
    "#! git clone https://github.com/someshsingh22/Sherlocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "from argparse import Namespace\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = Namespace(\n",
    "    train_file=\"../input/cano.txt\",\n",
    "    seq_size=64,\n",
    "    batch_size=256,\n",
    "    embedding_size=1024,\n",
    "    lstm_size=1024,\n",
    "    gradients_norm=5,\n",
    "    initial_words=['I', 'am'],\n",
    "    predict_top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(train_file, batch_size, seq_size):\n",
    "    with open(train_file, 'r') as f:\n",
    "        text = f.read()\n",
    "    text = text.split()\n",
    "\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
    "    n_vocab = len(int_to_vocab)\n",
    "\n",
    "    print('Vocabulary size', n_vocab)\n",
    "\n",
    "    int_text = [vocab_to_int[w] for w in text]\n",
    "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
    "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
    "    out_text = np.zeros_like(in_text)\n",
    "    out_text[:-1] = in_text[1:]\n",
    "    out_text[-1] = in_text[0]\n",
    "    in_text = np.reshape(in_text, (batch_size, -1))\n",
    "    out_text = np.reshape(out_text, (batch_size, -1))\n",
    "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,lstm_size,batch_first=True,num_layers=2,dropout=0.2)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "    \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "    \n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(2, batch_size, self.lstm_size),\n",
    "                torch.zeros(2, batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_train_op(net, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 21252\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(flags.train_file, flags.batch_size, flags.seq_size)\n",
    "net = RNNModule(n_vocab, flags.seq_size,flags.embedding_size, flags.lstm_size)\n",
    "net = net.to(device)\n",
    "criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    net.eval()\n",
    "\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "    \n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "\n",
    "    words.append(int_to_vocab[choice])\n",
    "    for _ in range(40):\n",
    "        ix = torch.tensor([[choice]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(int_to_vocab[choice])\n",
    "\n",
    "    return ' '.join(words).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/150 Loss: 21.151498794555664 Time Taken : 30.79806923866272\n",
      "Epoch: 1/150 Loss: 15.387161254882812 Time Taken : 30.704171180725098\n",
      "Epoch: 2/150 Loss: 14.619292259216309 Time Taken : 30.700597524642944\n",
      "Epoch: 3/150 Loss: 14.61365795135498 Time Taken : 30.705626010894775\n",
      "Epoch: 4/150 Loss: 13.542423248291016 Time Taken : 30.701051950454712\n",
      "Epoch: 5/150 Loss: 14.439783096313477 Time Taken : 30.690541744232178\n",
      "Epoch: 6/150 Loss: 13.303353309631348 Time Taken : 30.694021224975586\n",
      "Epoch: 7/150 Loss: 12.390857696533203 Time Taken : 30.70057964324951\n",
      "Epoch: 8/150 Loss: 11.845221519470215 Time Taken : 30.6942081451416\n",
      "-----\n",
      " b'Wait for me I I but but I and was was was but but was but was his I and his but and and and his but his and but and was but was was was I was his and but I his was' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his . his his was his I I I and was but was and I and and I but but and his but was and his I but his was his but his his and but but I was was was and' \n",
      "-----\n",
      "Epoch: 9/150 Loss: 11.709312438964844 Time Taken : 30.790343046188354\n",
      "Epoch: 10/150 Loss: 11.891919136047363 Time Taken : 30.69120216369629\n",
      "Epoch: 11/150 Loss: 11.368593215942383 Time Taken : 30.69047236442566\n",
      "Epoch: 12/150 Loss: 11.9210205078125 Time Taken : 30.690966606140137\n",
      "Epoch: 13/150 Loss: 11.398128509521484 Time Taken : 30.679880619049072\n",
      "Epoch: 14/150 Loss: 10.832584381103516 Time Taken : 30.692102432250977\n",
      "Epoch: 15/150 Loss: 10.77094841003418 Time Taken : 30.689898252487183\n",
      "Epoch: 16/150 Loss: 11.282965660095215 Time Taken : 30.6873836517334\n",
      "Epoch: 17/150 Loss: 10.564922332763672 Time Taken : 30.694947719573975\n",
      "Epoch: 18/150 Loss: 10.955427169799805 Time Taken : 30.686616897583008\n",
      "-----\n",
      " b'Wait for me the me you had the you the had have me the have me had have the have me had have you the the you me have have the you had have me had me had had me you me the had' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his up you you you have have have have had me you the me have you me the have you had you the you have have have have the me have had the had you had had have you me the me' \n",
      "-----\n",
      "Epoch: 19/150 Loss: 11.035741806030273 Time Taken : 30.759368896484375\n",
      "Epoch: 20/150 Loss: 10.767292022705078 Time Taken : 30.678158283233643\n",
      "Epoch: 21/150 Loss: 10.527654647827148 Time Taken : 30.67709732055664\n",
      "Epoch: 22/150 Loss: 10.27006721496582 Time Taken : 30.68094754219055\n",
      "Epoch: 23/150 Loss: 10.594541549682617 Time Taken : 30.672111749649048\n",
      "Epoch: 24/150 Loss: 9.726469039916992 Time Taken : 30.67366647720337\n",
      "Epoch: 25/150 Loss: 10.460958480834961 Time Taken : 30.680849075317383\n",
      "Epoch: 26/150 Loss: 9.795204162597656 Time Taken : 30.67661714553833\n",
      "Epoch: 27/150 Loss: 9.505775451660156 Time Taken : 30.681413173675537\n",
      "Epoch: 28/150 Loss: 9.411101341247559 Time Taken : 30.68123197555542\n",
      "-----\n",
      " b'Wait for me he the he the he the he his the in his he his the in she the the he his the the the in the his he his in she she she the she he she the in in in in' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his coat do his the the he he Holmes in in Holmes his in Holmes Holmes Holmes he in his in Holmes his in the he in he he the his in in his his he the Holmes Holmes he in in' \n",
      "-----\n",
      "Epoch: 29/150 Loss: 9.460021018981934 Time Taken : 30.753429412841797\n",
      "Epoch: 30/150 Loss: 9.559769630432129 Time Taken : 30.67583990097046\n",
      "Epoch: 31/150 Loss: 9.68852424621582 Time Taken : 30.6842360496521\n",
      "Epoch: 32/150 Loss: 9.87144660949707 Time Taken : 30.678218603134155\n",
      "Epoch: 33/150 Loss: 9.679377555847168 Time Taken : 30.6847403049469\n",
      "Epoch: 34/150 Loss: 9.478246688842773 Time Taken : 30.681350231170654\n",
      "Epoch: 35/150 Loss: 9.419326782226562 Time Taken : 30.67659878730774\n",
      "Epoch: 36/150 Loss: 9.489789962768555 Time Taken : 30.68039870262146\n",
      "Epoch: 37/150 Loss: 9.264909744262695 Time Taken : 30.67982292175293\n",
      "Epoch: 38/150 Loss: 9.270553588867188 Time Taken : 30.67355179786682\n",
      "-----\n",
      " b'Wait for me with I have have I have that have have , , have have have , with , I that with have that I with with that have that have with I that with I that have with with I I with' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his movements a with that that with , I that , with I I with with that with I with , that I with have have I have with that that with with , that that have with have that I with' \n",
      "-----\n",
      "Epoch: 39/150 Loss: 9.27694320678711 Time Taken : 30.74151349067688\n",
      "Epoch: 40/150 Loss: 9.103822708129883 Time Taken : 30.679683208465576\n",
      "Epoch: 41/150 Loss: 8.900489807128906 Time Taken : 30.67948603630066\n",
      "Epoch: 42/150 Loss: 8.70681095123291 Time Taken : 30.692468643188477\n",
      "Epoch: 43/150 Loss: 8.569938659667969 Time Taken : 30.684043407440186\n",
      "Epoch: 44/150 Loss: 8.680787086486816 Time Taken : 30.685065746307373\n",
      "Epoch: 45/150 Loss: 8.511897087097168 Time Taken : 30.676149606704712\n",
      "Epoch: 46/150 Loss: 8.451340675354004 Time Taken : 30.677672624588013\n",
      "Epoch: 47/150 Loss: 8.646587371826172 Time Taken : 30.675211668014526\n",
      "Epoch: 48/150 Loss: 8.106525421142578 Time Taken : 30.678293704986572\n",
      "-----\n",
      " b'Wait for me to I to to to I to to you you with to you with you with which I I to to you which to which with with which with to which I which you with you to which with I with' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his was I you with to his to with you to his I his his with I his with you to you with with with I you you his I to with you to I I his his his his with you' \n",
      "-----\n",
      "Epoch: 49/150 Loss: 8.307554244995117 Time Taken : 30.752412796020508\n",
      "Epoch: 50/150 Loss: 8.098808288574219 Time Taken : 30.67022204399109\n",
      "Epoch: 51/150 Loss: 7.971738815307617 Time Taken : 30.676241159439087\n",
      "Epoch: 52/150 Loss: 7.851471900939941 Time Taken : 30.683090925216675\n",
      "Epoch: 53/150 Loss: 7.844860553741455 Time Taken : 30.670605659484863\n",
      "Epoch: 54/150 Loss: 7.540267467498779 Time Taken : 30.68029761314392\n",
      "Epoch: 55/150 Loss: 7.450318336486816 Time Taken : 30.683515310287476\n",
      "Epoch: 56/150 Loss: 7.292289733886719 Time Taken : 30.68670654296875\n",
      "Epoch: 57/150 Loss: 7.337060928344727 Time Taken : 30.67530655860901\n",
      "Epoch: 58/150 Loss: 7.191136360168457 Time Taken : 30.67246699333191\n",
      "-----\n",
      " b'Wait for me I . , . , . I a . I a , I , , , . , of . of . , . I a I a . I . a of . , I . , I I a' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his and the of a of I I of , a a a a of , , . . a . , of . a , a . . , a a . I of I , a a of . a' \n",
      "-----\n",
      "Epoch: 59/150 Loss: 7.140851974487305 Time Taken : 30.748565435409546\n",
      "Epoch: 60/150 Loss: 7.186069488525391 Time Taken : 30.685722589492798\n",
      "Epoch: 61/150 Loss: 7.068739414215088 Time Taken : 30.681052446365356\n",
      "Epoch: 62/150 Loss: 6.849822521209717 Time Taken : 30.68946862220764\n",
      "Epoch: 63/150 Loss: 6.822142601013184 Time Taken : 30.686296939849854\n",
      "Epoch: 64/150 Loss: 6.75120735168457 Time Taken : 30.69657802581787\n",
      "Epoch: 65/150 Loss: 6.627876281738281 Time Taken : 30.689322471618652\n",
      "Epoch: 66/150 Loss: 6.553646087646484 Time Taken : 30.728264808654785\n",
      "Epoch: 67/150 Loss: 6.527256965637207 Time Taken : 30.71138310432434\n",
      "Epoch: 68/150 Loss: 6.5442118644714355 Time Taken : 30.737030744552612\n",
      "-----\n",
      " b'Wait for me . What the of have have . I to . You the have . You I have the the I the the of the of have have of . Holmes have . He I of of have . Holmes . I' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his . Holmes Holmes I the . Holmes have . A have I . I to I to I . You to I to . What the have the have to . You I . You , . I have have to' \n",
      "-----\n",
      "Epoch: 69/150 Loss: 6.414905071258545 Time Taken : 30.843153715133667\n",
      "Epoch: 70/150 Loss: 6.272311210632324 Time Taken : 30.77525520324707\n",
      "Epoch: 71/150 Loss: 5.818828582763672 Time Taken : 30.890236139297485\n",
      "Epoch: 72/150 Loss: 5.421176910400391 Time Taken : 30.867684602737427\n",
      "Epoch: 73/150 Loss: 5.28045129776001 Time Taken : 30.868968725204468\n",
      "Epoch: 74/150 Loss: 5.096773624420166 Time Taken : 30.91613459587097\n",
      "Epoch: 75/150 Loss: 5.027400970458984 Time Taken : 30.89769196510315\n",
      "Epoch: 76/150 Loss: 5.010962009429932 Time Taken : 30.870240926742554\n",
      "Epoch: 77/150 Loss: 4.848508834838867 Time Taken : 30.880803108215332\n",
      "Epoch: 78/150 Loss: 4.868870735168457 Time Taken : 30.910226583480835\n",
      "-----\n",
      " b'Wait for me that the road had a man and the north , but I could do , said he . He is not a most different yards to be the house to have the north which I will look in the most extraordinary' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his hand in the moor . It is no doubt . I have not seen it ? It may be in his face , said the Hall and a bicycle as a bicycle to the road , and I was in the' \n",
      "-----\n",
      "Epoch: 79/150 Loss: 4.746540546417236 Time Taken : 30.96700096130371\n",
      "Epoch: 80/150 Loss: 4.68638277053833 Time Taken : 30.86229395866394\n",
      "Epoch: 81/150 Loss: 4.60386323928833 Time Taken : 30.90381169319153\n",
      "Epoch: 82/150 Loss: 4.5263237953186035 Time Taken : 30.886398553848267\n",
      "Epoch: 83/150 Loss: 4.45037841796875 Time Taken : 30.895007133483887\n",
      "Epoch: 84/150 Loss: 4.371036052703857 Time Taken : 30.884254693984985\n",
      "Epoch: 85/150 Loss: 4.3184309005737305 Time Taken : 30.893741607666016\n",
      "Epoch: 86/150 Loss: 4.2703633308410645 Time Taken : 30.898823261260986\n",
      "Epoch: 87/150 Loss: 4.204099655151367 Time Taken : 30.854050159454346\n",
      "Epoch: 88/150 Loss: 4.133607864379883 Time Taken : 30.87368154525757\n",
      "-----\n",
      " b'Wait for me some years ago than ever . It would have to be very different , and a long . He has been taken up and bizarre in a different and one of the same man with her hand , he answered with' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his out shoulder at a mans master side , but he was not in his own hand . Then she could be the same and I saw him that he could have seen him . Then the most singular . I could' \n",
      "-----\n",
      "Epoch: 89/150 Loss: 4.087406158447266 Time Taken : 30.951813220977783\n",
      "Epoch: 90/150 Loss: 4.042769432067871 Time Taken : 30.843772649765015\n",
      "Epoch: 91/150 Loss: 4.0053558349609375 Time Taken : 30.842144012451172\n",
      "Epoch: 92/150 Loss: 3.9590401649475098 Time Taken : 30.85929036140442\n",
      "Epoch: 93/150 Loss: 3.9160194396972656 Time Taken : 30.867905616760254\n",
      "Epoch: 94/150 Loss: 3.871361017227173 Time Taken : 30.879389762878418\n",
      "Epoch: 95/150 Loss: 3.845345973968506 Time Taken : 30.86066460609436\n",
      "Epoch: 96/150 Loss: 3.7954397201538086 Time Taken : 30.87306785583496\n",
      "Epoch: 97/150 Loss: 3.754971504211426 Time Taken : 30.848448514938354\n",
      "Epoch: 98/150 Loss: 3.707430601119995 Time Taken : 30.85395336151123\n",
      "-----\n",
      " b'Wait for me the folk . But if she came in me , however in a small and barrel , as I was glad that it was Paul ! he asked him . You are a little more in a circle , with some' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his hand on our hand . Then I saw that he is the most unique problem , said Holmes with an hour , and I should think that it should seem to be no further more of a thousand pounds in this' \n",
      "-----\n",
      "Epoch: 99/150 Loss: 3.6895453929901123 Time Taken : 30.91183638572693\n",
      "Epoch: 100/150 Loss: 3.6465070247650146 Time Taken : 30.85505485534668\n",
      "Epoch: 101/150 Loss: 3.5922703742980957 Time Taken : 30.859199285507202\n",
      "Epoch: 102/150 Loss: 3.56852388381958 Time Taken : 30.87174105644226\n",
      "Epoch: 103/150 Loss: 3.531312942504883 Time Taken : 30.855347633361816\n",
      "Epoch: 104/150 Loss: 3.535658597946167 Time Taken : 30.87738347053528\n",
      "Epoch: 105/150 Loss: 3.4884910583496094 Time Taken : 30.852020978927612\n",
      "Epoch: 106/150 Loss: 3.4746170043945312 Time Taken : 30.8790225982666\n",
      "Epoch: 107/150 Loss: 3.439000368118286 Time Taken : 30.868459701538086\n",
      "Epoch: 108/150 Loss: 3.417510747909546 Time Taken : 30.861814498901367\n",
      "-----\n",
      " b'Wait for me not a long , but it seemed to be rather a cry . I was a bit , but it is not for a moment , and the humbler have no alternative to see you in town . Barrymore was a' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his . There is nothing ? He was on that dark figure that the girl had come from a year . I saw the very edge for the first night before the two people of the very morning . He is the' \n",
      "-----\n",
      "Epoch: 109/150 Loss: 3.375816822052002 Time Taken : 30.896650075912476\n",
      "Epoch: 110/150 Loss: 3.3568804264068604 Time Taken : 30.858487606048584\n",
      "Epoch: 111/150 Loss: 3.320934534072876 Time Taken : 30.852994918823242\n",
      "Epoch: 112/150 Loss: 3.300910711288452 Time Taken : 30.831875801086426\n",
      "Epoch: 113/150 Loss: 3.276031017303467 Time Taken : 30.849822759628296\n",
      "Epoch: 114/150 Loss: 3.2326924800872803 Time Taken : 30.80817151069641\n",
      "Epoch: 115/150 Loss: 3.2332565784454346 Time Taken : 30.859810829162598\n",
      "Epoch: 116/150 Loss: 3.213284969329834 Time Taken : 30.834927797317505\n",
      "Epoch: 117/150 Loss: 3.2006547451019287 Time Taken : 30.801254272460938\n",
      "Epoch: 118/150 Loss: 3.164257764816284 Time Taken : 30.82623839378357\n",
      "-----\n",
      " b'Wait for me had he heard a sort to be the father had given them . He came to the foot upon the stairs I came out , with the creasote door , where laborers , was precious , as he was a very' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his ? his hands ? he drew it from her hand towards his bed ; he could not be imagined that he would never be seen in the house before , Mr ? he stammered with a slight laugh ? As I' \n",
      "-----\n",
      "Epoch: 119/150 Loss: 3.156132221221924 Time Taken : 30.90501356124878\n",
      "Epoch: 120/150 Loss: 3.1144847869873047 Time Taken : 30.824044942855835\n",
      "Epoch: 121/150 Loss: 3.091219425201416 Time Taken : 30.819043397903442\n",
      "Epoch: 122/150 Loss: 3.0697503089904785 Time Taken : 30.834590196609497\n",
      "Epoch: 123/150 Loss: 3.040990114212036 Time Taken : 30.81575059890747\n",
      "Epoch: 124/150 Loss: 3.0264713764190674 Time Taken : 30.792026042938232\n",
      "Epoch: 125/150 Loss: 2.990638017654419 Time Taken : 30.83825159072876\n",
      "Epoch: 126/150 Loss: 2.967653751373291 Time Taken : 30.80426049232483\n",
      "Epoch: 127/150 Loss: 2.9501852989196777 Time Taken : 30.851308822631836\n",
      "Epoch: 128/150 Loss: 2.925067901611328 Time Taken : 30.841692447662354\n",
      "-----\n",
      " b'Wait for me . I shall send up your letter . Ah ! here will you allowed to know the facts in your doubts . I will send her you as a very careful look in bewilderment over these standing in their defence .' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his me with the pistol ! I asked , my boy to be of importance ? He had started on his desk under cover side on his knees , with an absent , was the same who had driven him to the' \n",
      "-----\n",
      "Epoch: 129/150 Loss: 2.910663604736328 Time Taken : 30.92562437057495\n",
      "Epoch: 130/150 Loss: 2.898439884185791 Time Taken : 30.837154865264893\n",
      "Epoch: 131/150 Loss: 2.8783581256866455 Time Taken : 30.843610048294067\n",
      "Epoch: 132/150 Loss: 2.8310177326202393 Time Taken : 30.84934377670288\n",
      "Epoch: 133/150 Loss: 2.829472541809082 Time Taken : 30.830082416534424\n",
      "Epoch: 134/150 Loss: 2.808842420578003 Time Taken : 30.8287410736084\n",
      "Epoch: 135/150 Loss: 2.785761594772339 Time Taken : 30.84835457801819\n",
      "Epoch: 136/150 Loss: 2.771028518676758 Time Taken : 30.837059497833252\n",
      "Epoch: 137/150 Loss: 2.7368857860565186 Time Taken : 30.8681480884552\n",
      "Epoch: 138/150 Loss: 2.725571870803833 Time Taken : 30.858541011810303\n",
      "-----\n",
      " b'Wait for me what I had a secret to it . To my variety , of the pungent boulders were waiting in regard , said Sherlock Holmes quietly to my amazement . You are too timid to see you in this final importance .' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his the hand on his pistol . , Hopkins make it very bad that we were half the effect that I was bound to tell you how you are the best word that you were leaving any unnecessary oaths of the Duke' \n",
      "-----\n",
      "Epoch: 139/150 Loss: 2.705986499786377 Time Taken : 30.935369729995728\n",
      "Epoch: 140/150 Loss: 2.681988000869751 Time Taken : 30.810407876968384\n",
      "Epoch: 141/150 Loss: 2.678605318069458 Time Taken : 30.843881607055664\n",
      "Epoch: 142/150 Loss: 2.647336483001709 Time Taken : 30.868529558181763\n",
      "Epoch: 143/150 Loss: 2.636138439178467 Time Taken : 30.822395086288452\n",
      "Epoch: 144/150 Loss: 2.6138687133789062 Time Taken : 30.836217880249023\n",
      "Epoch: 145/150 Loss: 2.600391387939453 Time Taken : 30.85098099708557\n",
      "Epoch: 146/150 Loss: 2.6000192165374756 Time Taken : 30.807498693466187\n",
      "Epoch: 147/150 Loss: 2.5571107864379883 Time Taken : 30.829075813293457\n",
      "Epoch: 148/150 Loss: 2.5281364917755127 Time Taken : 30.83952045440674\n",
      "-----\n",
      " b'Wait for me you on a boy who knew him a sort . He thrust him across , who had thrust its into my power by that part from his father . Thats the end to you and the inspector with variety upon that' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his Ernests my hand , lying blood away , and the pistol between the heath with his face resting upon my soul ; he came to the lodge , and that my husband could not be moved for a minute or dark' \n",
      "-----\n",
      "Epoch: 149/150 Loss: 2.521686553955078 Time Taken : 30.913738250732422\n"
     ]
    }
   ],
   "source": [
    "call=0\n",
    "epochs=150\n",
    "for e in range(epochs):\n",
    "    call+=1\n",
    "    start=time.time()\n",
    "    batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
    "    state_h, state_c = net.zero_state(flags.batch_size)\n",
    "\n",
    "    # Transfer data to GPU\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for x, y in batches:\n",
    "        iteration += 1\n",
    "\n",
    "        # Tell it we are in training mode\n",
    "        net.train()\n",
    "\n",
    "        # Reset all gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Transfer data to GPU\n",
    "        x = torch.tensor(x).to(device)\n",
    "        y = torch.tensor(y).to(device)\n",
    "\n",
    "        logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "        loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "        state_h = state_h.detach()\n",
    "        state_c = state_c.detach()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        # Update the network's parameters\n",
    "        optimizer.step()\n",
    "        loss.backward()\n",
    "\n",
    "        _ = torch.nn.utils.clip_grad_norm_(net.parameters(), flags.gradients_norm)\n",
    "        optimizer.step()\n",
    "    if(call%10==0):\n",
    "        x=predict(device, net, \"Wait for me\".split(), n_vocab, vocab_to_int, int_to_vocab, top_k=5)\n",
    "        y=predict(device, net, \"Sherlock rubbed his\".split(), n_vocab, vocab_to_int, int_to_vocab, top_k=5)\n",
    "        print(\"-----\\n\",x,\"\\n-----\")\n",
    "        print(\"-----\\n\",y,\"\\n-----\")\n",
    "    print('Epoch: {}/{}'.format(e, epochs),'Loss: {}'.format(loss_value),'Time Taken : {}'.format(time.time()-start))\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] *= 0.99\n",
    "    if loss_value<=0.75:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = Namespace(\n",
    "    train_file=\"../input/cano.txt\",\n",
    "    seq_size=64,\n",
    "    batch_size=256,\n",
    "    embedding_size=512,\n",
    "    lstm_size=512,\n",
    "    gradients_norm=5,\n",
    "    initial_words=['I', 'am'],\n",
    "    predict_top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(train_file, batch_size, seq_size):\n",
    "    with open(train_file, 'r') as f:\n",
    "        text = f.read()\n",
    "    text = text.split()\n",
    "\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
    "    n_vocab = len(int_to_vocab)\n",
    "\n",
    "    print('Vocabulary size', n_vocab)\n",
    "\n",
    "    int_text = [vocab_to_int[w] for w in text]\n",
    "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
    "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
    "    out_text = np.zeros_like(in_text)\n",
    "    out_text[:-1] = in_text[1:]\n",
    "    out_text[-1] = in_text[0]\n",
    "    in_text = np.reshape(in_text, (batch_size, -1))\n",
    "    out_text = np.reshape(out_text, (batch_size, -1))\n",
    "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,lstm_size,batch_first=True,num_layers=2,dropout=0.2)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "    \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "    \n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(2, batch_size, self.lstm_size),\n",
    "                torch.zeros(2, batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_train_op(net, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 21252\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(flags.train_file, flags.batch_size, flags.seq_size)\n",
    "net = RNNModule(n_vocab, flags.seq_size,flags.embedding_size, flags.lstm_size)\n",
    "net = net.to(device)\n",
    "criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/150 Loss: 7.83593225479126 Time Taken : 17.747049570083618\n",
      "Epoch: 1/150 Loss: 7.407349109649658 Time Taken : 17.769440412521362\n",
      "Epoch: 2/150 Loss: 7.672333240509033 Time Taken : 17.766834259033203\n",
      "Epoch: 3/150 Loss: 7.494349002838135 Time Taken : 17.767624139785767\n",
      "Epoch: 4/150 Loss: 7.570348739624023 Time Taken : 17.767653703689575\n",
      "Epoch: 5/150 Loss: 7.573446750640869 Time Taken : 17.766326665878296\n",
      "Epoch: 6/150 Loss: 7.751138210296631 Time Taken : 17.764432907104492\n",
      "Epoch: 7/150 Loss: 7.373069763183594 Time Taken : 17.77657389640808\n",
      "Epoch: 8/150 Loss: 7.513638973236084 Time Taken : 17.771028757095337\n",
      "-----\n",
      " b'Wait for me in the the of the and you the and in you and in the the in you and of you in and you of the the you you in and the you in the of in in the the you of' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his the in of you in in you and you you of the and in the you you of of of and the of in you and in the you and you you the in in in the and and in and' \n",
      "-----\n",
      "Epoch: 9/150 Loss: 7.34362268447876 Time Taken : 17.82781195640564\n",
      "Epoch: 10/150 Loss: 7.416192531585693 Time Taken : 17.752079486846924\n",
      "Epoch: 11/150 Loss: 7.145366668701172 Time Taken : 17.754746675491333\n",
      "Epoch: 12/150 Loss: 7.007933616638184 Time Taken : 17.751923322677612\n",
      "Epoch: 13/150 Loss: 7.1622772216796875 Time Taken : 17.744643926620483\n",
      "Epoch: 14/150 Loss: 7.043240070343018 Time Taken : 17.749571084976196\n",
      "Epoch: 15/150 Loss: 6.852779388427734 Time Taken : 17.75422191619873\n",
      "Epoch: 16/150 Loss: 6.934410095214844 Time Taken : 17.76609230041504\n",
      "Epoch: 17/150 Loss: 6.965420722961426 Time Taken : 17.752290964126587\n",
      "Epoch: 18/150 Loss: 6.946871757507324 Time Taken : 17.75432062149048\n",
      "-----\n",
      " b'Wait for me , he for he ? for . . ? ? ? It he he the for ? for the . for ? he . he is ? It that that he ? It he . It ? ? he . .' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his ? It ? . ? for that the . ? It that the ? . for the ? . he the It that he . ? It the for . It . . for that that ? It the he ?' \n",
      "-----\n",
      "Epoch: 19/150 Loss: 7.085745334625244 Time Taken : 17.811655282974243\n",
      "Epoch: 20/150 Loss: 7.139623641967773 Time Taken : 17.74556589126587\n",
      "Epoch: 21/150 Loss: 7.1365132331848145 Time Taken : 17.7508602142334\n",
      "Epoch: 22/150 Loss: 7.100094795227051 Time Taken : 17.755218982696533\n",
      "Epoch: 23/150 Loss: 7.136457443237305 Time Taken : 17.748175382614136\n",
      "Epoch: 24/150 Loss: 7.113589286804199 Time Taken : 17.743714332580566\n",
      "Epoch: 25/150 Loss: 7.37001895904541 Time Taken : 17.745904207229614\n",
      "Epoch: 26/150 Loss: 7.416174411773682 Time Taken : 17.741899490356445\n",
      "Epoch: 27/150 Loss: 7.320492267608643 Time Taken : 17.75590229034424\n",
      "Epoch: 28/150 Loss: 7.15312385559082 Time Taken : 17.74987483024597\n",
      "-----\n",
      " b'Wait for me the at , , , the be I the to the , I to I the I I the , to , the be the , the the the be is the the , is to , to , the be' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his at , , I is , is to the is is , I is is to the to , to to I I , I , I , , is to to the is to to the to to is ,' \n",
      "-----\n",
      "Epoch: 29/150 Loss: 7.244337558746338 Time Taken : 17.81250309944153\n",
      "Epoch: 30/150 Loss: 6.932489395141602 Time Taken : 17.75504446029663\n",
      "Epoch: 31/150 Loss: 6.839060306549072 Time Taken : 17.763617515563965\n",
      "Epoch: 32/150 Loss: 6.813467025756836 Time Taken : 17.758923530578613\n",
      "Epoch: 33/150 Loss: 6.822916507720947 Time Taken : 17.753473043441772\n",
      "Epoch: 34/150 Loss: 6.680619239807129 Time Taken : 17.757040977478027\n",
      "Epoch: 35/150 Loss: 6.760844707489014 Time Taken : 17.760655641555786\n",
      "Epoch: 36/150 Loss: 6.623067378997803 Time Taken : 17.75528836250305\n",
      "Epoch: 37/150 Loss: 6.543761730194092 Time Taken : 17.752513647079468\n",
      "Epoch: 38/150 Loss: 6.512521743774414 Time Taken : 17.758445024490356\n",
      "-----\n",
      " b'Wait for me of , a and and I the , the a , a I I the I the the I the the I and , I and I the the , the I I the a I and a and the the' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his , , the , and and and the , and I , the of , , , , the , and , I I , I of , the and the I of to of , , and the of to' \n",
      "-----\n",
      "Epoch: 39/150 Loss: 6.453620433807373 Time Taken : 17.816073894500732\n",
      "Epoch: 40/150 Loss: 6.549417972564697 Time Taken : 17.74583649635315\n",
      "Epoch: 41/150 Loss: 6.548545837402344 Time Taken : 17.74842667579651\n",
      "Epoch: 42/150 Loss: 6.5470662117004395 Time Taken : 17.74602437019348\n",
      "Epoch: 43/150 Loss: 6.515210151672363 Time Taken : 17.7544424533844\n",
      "Epoch: 44/150 Loss: 6.539078712463379 Time Taken : 17.750807285308838\n",
      "Epoch: 45/150 Loss: 6.372159481048584 Time Taken : 17.748525142669678\n",
      "Epoch: 46/150 Loss: 6.2977495193481445 Time Taken : 17.749390363693237\n",
      "Epoch: 47/150 Loss: 6.314166069030762 Time Taken : 17.749173402786255\n",
      "Epoch: 48/150 Loss: 6.286920070648193 Time Taken : 17.74672532081604\n",
      "-----\n",
      " b'Wait for me for Holmes . Well to the the he and he he not be the and not and and and and be and not and he the the not and the be . You and he and and he be and be' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his he the and he not be . It be the not he he be a and and and be the not the be , be to not he and the not not be , he be a and and he he' \n",
      "-----\n",
      "Epoch: 49/150 Loss: 6.247069358825684 Time Taken : 17.81543731689453\n",
      "Epoch: 50/150 Loss: 6.397399425506592 Time Taken : 17.752501487731934\n",
      "Epoch: 51/150 Loss: 6.411266326904297 Time Taken : 17.753633737564087\n",
      "Epoch: 52/150 Loss: 6.4038567543029785 Time Taken : 17.755789279937744\n",
      "Epoch: 53/150 Loss: 6.359579563140869 Time Taken : 17.756290674209595\n",
      "Epoch: 54/150 Loss: 6.220629692077637 Time Taken : 17.754480361938477\n",
      "Epoch: 55/150 Loss: 5.983968734741211 Time Taken : 17.75633215904236\n",
      "Epoch: 56/150 Loss: 5.765530586242676 Time Taken : 17.756293535232544\n",
      "Epoch: 57/150 Loss: 5.654331684112549 Time Taken : 17.754991054534912\n",
      "Epoch: 58/150 Loss: 5.634115219116211 Time Taken : 17.75788402557373\n",
      "-----\n",
      " b'Wait for me . Now in the very face to I have have you had not the man to be in that the door had be in a very face , but you could be the man in that be , I could it' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his county had . Holmes , was a most face in that the chair , but be the man to the most long face in that you could be a most most most most face in it of my face . Now' \n",
      "-----\n",
      "Epoch: 59/150 Loss: 5.543854713439941 Time Taken : 17.822575092315674\n",
      "Epoch: 60/150 Loss: 5.436389446258545 Time Taken : 17.755184412002563\n",
      "Epoch: 61/150 Loss: 5.429321765899658 Time Taken : 17.752153158187866\n",
      "Epoch: 62/150 Loss: 5.3451056480407715 Time Taken : 17.759002685546875\n",
      "Epoch: 63/150 Loss: 5.2942118644714355 Time Taken : 17.750116109848022\n",
      "Epoch: 64/150 Loss: 5.242708206176758 Time Taken : 17.74713945388794\n",
      "Epoch: 65/150 Loss: 5.163379192352295 Time Taken : 17.749345541000366\n",
      "Epoch: 66/150 Loss: 5.146572113037109 Time Taken : 17.74880838394165\n",
      "Epoch: 67/150 Loss: 5.071621417999268 Time Taken : 17.75160002708435\n",
      "Epoch: 68/150 Loss: 5.041805267333984 Time Taken : 17.75064468383789\n",
      "-----\n",
      " b'Wait for me and a large and so . The case was to the matter with my mind . He had made me to be , but you I had a little , said I in this man , said he , said he' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his name , and he could it will have have have heard , and it had have heard a long smile of one , but it is a fine man . Now . He could a little smile , but I had' \n",
      "-----\n",
      "Epoch: 69/150 Loss: 4.995070457458496 Time Taken : 17.80965518951416\n",
      "Epoch: 70/150 Loss: 4.963639259338379 Time Taken : 17.775031328201294\n",
      "Epoch: 71/150 Loss: 4.909204959869385 Time Taken : 17.77006959915161\n",
      "Epoch: 72/150 Loss: 4.887923240661621 Time Taken : 17.767303943634033\n",
      "Epoch: 73/150 Loss: 4.8760905265808105 Time Taken : 17.782768726348877\n",
      "Epoch: 74/150 Loss: 4.79244327545166 Time Taken : 17.771231174468994\n",
      "Epoch: 75/150 Loss: 4.7939324378967285 Time Taken : 17.77122402191162\n",
      "Epoch: 76/150 Loss: 4.747927188873291 Time Taken : 17.771584510803223\n",
      "Epoch: 77/150 Loss: 4.711048603057861 Time Taken : 17.770329236984253\n",
      "Epoch: 78/150 Loss: 4.7020583152771 Time Taken : 17.770657062530518\n",
      "-----\n",
      " b'Wait for me , and I think I could tell you , he said that he was a man . He could do him in him in a little man which would do in a little instant , the doctor , he had always' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his bed , Holmes , but he would be in his hands and in it ? Yes ? Yes . He has been not in his hand . But the lady would the police , he remarked that you will the other' \n",
      "-----\n",
      "Epoch: 79/150 Loss: 4.679614067077637 Time Taken : 17.83578372001648\n",
      "Epoch: 80/150 Loss: 4.631052494049072 Time Taken : 17.747585773468018\n",
      "Epoch: 81/150 Loss: 4.625014781951904 Time Taken : 17.755488872528076\n",
      "Epoch: 82/150 Loss: 4.582108497619629 Time Taken : 17.75145411491394\n",
      "Epoch: 83/150 Loss: 4.584899425506592 Time Taken : 17.76035213470459\n",
      "Epoch: 84/150 Loss: 4.565464496612549 Time Taken : 17.752329349517822\n",
      "Epoch: 85/150 Loss: 4.536949157714844 Time Taken : 17.754172563552856\n",
      "Epoch: 86/150 Loss: 4.503543376922607 Time Taken : 17.75877833366394\n",
      "Epoch: 87/150 Loss: 4.498331546783447 Time Taken : 17.746800184249878\n",
      "Epoch: 88/150 Loss: 4.460389137268066 Time Taken : 17.74899172782898\n",
      "-----\n",
      " b'Wait for me , a few months ago . I think . It was as the lady ? Yes , that is no use for her . My friend was in a man which could tell us it , and she was a very' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his Mr horribly and had a woman and a cap of Sumatra figure in our friend cap . But he is not in his death . The lady had come to have the family . He has seen him to get a' \n",
      "-----\n",
      "Epoch: 89/150 Loss: 4.446836948394775 Time Taken : 17.810431718826294\n",
      "Epoch: 90/150 Loss: 4.393924713134766 Time Taken : 17.75085711479187\n",
      "Epoch: 91/150 Loss: 4.380680084228516 Time Taken : 17.748201370239258\n",
      "Epoch: 92/150 Loss: 4.370386123657227 Time Taken : 17.749675512313843\n",
      "Epoch: 93/150 Loss: 4.3454694747924805 Time Taken : 17.74486231803894\n",
      "Epoch: 94/150 Loss: 4.325677871704102 Time Taken : 17.7489812374115\n",
      "Epoch: 95/150 Loss: 4.338653087615967 Time Taken : 17.753687381744385\n",
      "Epoch: 96/150 Loss: 4.31709098815918 Time Taken : 17.750105619430542\n",
      "Epoch: 97/150 Loss: 4.2668890953063965 Time Taken : 17.748778104782104\n",
      "Epoch: 98/150 Loss: 4.2375264167785645 Time Taken : 17.749042749404907\n",
      "-----\n",
      " b'Wait for me I could have recovered the house ? It is that the police will tell us to the cellar ? Because she was very very well aware , Mr . Carruthers is at my own sake ? Because he was not a' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his Mr with friend with his iron and his housekeeper , with the woods of a false interest as you think I am of his chair . I think you may come in the first time . He has come in the' \n",
      "-----\n",
      "Epoch: 99/150 Loss: 4.2279205322265625 Time Taken : 17.807738780975342\n",
      "Epoch: 100/150 Loss: 4.198326110839844 Time Taken : 17.755942344665527\n",
      "Epoch: 101/150 Loss: 4.186901569366455 Time Taken : 17.757569789886475\n",
      "Epoch: 102/150 Loss: 4.176276206970215 Time Taken : 17.75789785385132\n",
      "Epoch: 103/150 Loss: 4.171043872833252 Time Taken : 17.75910758972168\n",
      "Epoch: 104/150 Loss: 4.166304588317871 Time Taken : 17.76139235496521\n",
      "Epoch: 105/150 Loss: 4.134718894958496 Time Taken : 17.755857467651367\n",
      "Epoch: 106/150 Loss: 4.10690450668335 Time Taken : 17.75876021385193\n",
      "Epoch: 107/150 Loss: 4.095888614654541 Time Taken : 17.758976697921753\n",
      "Epoch: 108/150 Loss: 4.083685398101807 Time Taken : 17.75328755378723\n",
      "-----\n",
      " b'Wait for me in it seemed in a stain which might interject in the postman . The professor was in which he had recovered . He snuggled to be blamed for the postman ? Snarling aged lady drooped himself . The lecturer was annoyed' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his eyes Holmes In his glasses , and his eyes fell on a relation of life which was the relation . Well , you may be sorry to be in this room and in this room , but he could see it' \n",
      "-----\n",
      "Epoch: 109/150 Loss: 4.06680965423584 Time Taken : 17.817248582839966\n",
      "Epoch: 110/150 Loss: 4.051810264587402 Time Taken : 17.745184421539307\n",
      "Epoch: 111/150 Loss: 4.057581901550293 Time Taken : 17.75623393058777\n",
      "Epoch: 112/150 Loss: 4.01509428024292 Time Taken : 17.749451398849487\n",
      "Epoch: 113/150 Loss: 3.993513584136963 Time Taken : 17.752365350723267\n",
      "Epoch: 114/150 Loss: 3.980302095413208 Time Taken : 17.754788398742676\n",
      "Epoch: 115/150 Loss: 3.9447083473205566 Time Taken : 17.751240968704224\n",
      "Epoch: 116/150 Loss: 3.941044807434082 Time Taken : 17.749711275100708\n",
      "Epoch: 117/150 Loss: 3.930647611618042 Time Taken : 17.751384735107422\n",
      "Epoch: 118/150 Loss: 3.907827138900757 Time Taken : 17.751502752304077\n",
      "-----\n",
      " b'Wait for me my companion by Sumatra and the gravest volume that I have gained nothing in this point ? Is that you may be a fool , Watson , you can make you to do it to him ? I am in London' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his mind , a few hundred pounds . It would be sure I could not not see me , but he has a little more . I have been in my life to do , and it is the dog , said' \n",
      "-----\n",
      "Epoch: 119/150 Loss: 3.8893544673919678 Time Taken : 17.810123443603516\n",
      "Epoch: 120/150 Loss: 3.88181209564209 Time Taken : 17.757714986801147\n",
      "Epoch: 121/150 Loss: 3.8639743328094482 Time Taken : 17.753546953201294\n",
      "Epoch: 122/150 Loss: 3.853668451309204 Time Taken : 17.760936498641968\n",
      "Epoch: 123/150 Loss: 3.8393099308013916 Time Taken : 17.757949590682983\n",
      "Epoch: 124/150 Loss: 3.8240749835968018 Time Taken : 17.75793719291687\n",
      "Epoch: 125/150 Loss: 3.7887661457061768 Time Taken : 17.760300636291504\n",
      "Epoch: 126/150 Loss: 3.7706873416900635 Time Taken : 17.758209705352783\n",
      "Epoch: 127/150 Loss: 3.753413438796997 Time Taken : 17.755707502365112\n",
      "Epoch: 128/150 Loss: 3.7208261489868164 Time Taken : 17.761542558670044\n",
      "-----\n",
      " b'Wait for me in night , but I have been the richest spectators to cycle from a vacancy upon a penniless wound with Pall Mall , the cycle stain , which varies the same hour . It was a hundred miles to his associates' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his eyes eyes open the Sherlock Sherlock You would be to be in the world in a gentle , heavy , and he has done , but that is it , I have read the whole story as to my ears as' \n",
      "-----\n",
      "Epoch: 129/150 Loss: 3.705007791519165 Time Taken : 17.819305419921875\n",
      "Epoch: 130/150 Loss: 3.7000277042388916 Time Taken : 17.750017642974854\n",
      "Epoch: 131/150 Loss: 3.67012357711792 Time Taken : 17.750280618667603\n",
      "Epoch: 132/150 Loss: 3.652418375015259 Time Taken : 17.751703023910522\n",
      "Epoch: 133/150 Loss: 3.6539113521575928 Time Taken : 17.754276514053345\n",
      "Epoch: 134/150 Loss: 3.63419771194458 Time Taken : 17.75441861152649\n",
      "Epoch: 135/150 Loss: 3.6056835651397705 Time Taken : 17.752735137939453\n",
      "Epoch: 136/150 Loss: 3.5951027870178223 Time Taken : 17.7512149810791\n",
      "Epoch: 137/150 Loss: 3.584641218185425 Time Taken : 17.74749994277954\n",
      "Epoch: 138/150 Loss: 3.5630130767822266 Time Taken : 17.752548456192017\n",
      "-----\n",
      " b'Wait for me . He fired ship his head , with a laugh with a roar under its scattered Packet foolscap and the most singular qualities . It would do you to make you . He would do , and I could have no' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his researches bedroom and a , and , was , and with an instant over the snow of which he had ceased from a smile in that it had purchased it . He has shown his habits , with a long drive' \n",
      "-----\n",
      "Epoch: 139/150 Loss: 3.5448169708251953 Time Taken : 17.810261011123657\n",
      "Epoch: 140/150 Loss: 3.5545520782470703 Time Taken : 17.77224588394165\n",
      "Epoch: 141/150 Loss: 3.526291847229004 Time Taken : 17.772478818893433\n",
      "Epoch: 142/150 Loss: 3.529341220855713 Time Taken : 17.772936820983887\n",
      "Epoch: 143/150 Loss: 3.5101120471954346 Time Taken : 17.77492117881775\n",
      "Epoch: 144/150 Loss: 3.498889923095703 Time Taken : 17.773122787475586\n",
      "Epoch: 145/150 Loss: 3.4664320945739746 Time Taken : 17.7722225189209\n",
      "Epoch: 146/150 Loss: 3.475905656814575 Time Taken : 17.77433180809021\n",
      "Epoch: 147/150 Loss: 3.441293478012085 Time Taken : 17.775103330612183\n",
      "Epoch: 148/150 Loss: 3.4382174015045166 Time Taken : 17.764352321624756\n",
      "-----\n",
      " b'Wait for me that young friend that it is pretty singular up in order with Ridling qualities . She had given the loot end in his pockets upon drawers in a year of venerable doings . The drawers , however , which is to' \n",
      "-----\n",
      "-----\n",
      " b'Sherlock rubbed his and , the evening with a cloth , golden face turned over his chin in my own way . He has not gone to my hand , so it is very long in a whisper , for the only thing it' \n",
      "-----\n",
      "Epoch: 149/150 Loss: 3.435340404510498 Time Taken : 17.8327579498291\n"
     ]
    }
   ],
   "source": [
    "call=0\n",
    "epochs=150\n",
    "for e in range(epochs):\n",
    "    call+=1\n",
    "    start=time.time()\n",
    "    batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
    "    state_h, state_c = net.zero_state(flags.batch_size)\n",
    "\n",
    "    # Transfer data to GPU\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for x, y in batches:\n",
    "        iteration += 1\n",
    "\n",
    "        # Tell it we are in training mode\n",
    "        net.train()\n",
    "\n",
    "        # Reset all gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Transfer data to GPU\n",
    "        x = torch.tensor(x).to(device)\n",
    "        y = torch.tensor(y).to(device)\n",
    "\n",
    "        logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "        loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "        state_h = state_h.detach()\n",
    "        state_c = state_c.detach()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        # Update the network's parameters\n",
    "        optimizer.step()\n",
    "        loss.backward()\n",
    "\n",
    "        _ = torch.nn.utils.clip_grad_norm_(net.parameters(), flags.gradients_norm)\n",
    "        optimizer.step()\n",
    "    if(call%10==0):\n",
    "        x=predict(device, net, \"Wait for me\".split(), n_vocab, vocab_to_int, int_to_vocab, top_k=5)\n",
    "        y=predict(device, net, \"Sherlock rubbed his\".split(), n_vocab, vocab_to_int, int_to_vocab, top_k=5)\n",
    "        print(\"-----\\n\",x,\"\\n-----\")\n",
    "        print(\"-----\\n\",y,\"\\n-----\")\n",
    "    print('Epoch: {}/{}'.format(e, epochs),'Loss: {}'.format(loss_value),'Time Taken : {}'.format(time.time()-start))\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] *= 0.99\n",
    "    if loss_value<=0.75:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
