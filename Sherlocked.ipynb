{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sherlocked.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/someshsingh22/Sherlocked/blob/master/Sherlocked.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTVePy0RwXRE",
        "colab_type": "code",
        "outputId": "45c21d96-362a-47c0-ff8c-2dd62a39453f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "! git clone https://github.com/someshsingh22/Sherlocked"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Sherlocked'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/60)\u001b[K\rremote: Counting objects:   3% (2/60)\u001b[K\rremote: Counting objects:   5% (3/60)\u001b[K\rremote: Counting objects:   6% (4/60)\u001b[K\rremote: Counting objects:   8% (5/60)\u001b[K\rremote: Counting objects:  10% (6/60)\u001b[K\rremote: Counting objects:  11% (7/60)\u001b[K\rremote: Counting objects:  13% (8/60)\u001b[K\rremote: Counting objects:  15% (9/60)\u001b[K\rremote: Counting objects:  16% (10/60)\u001b[K\rremote: Counting objects:  18% (11/60)\u001b[K\rremote: Counting objects:  20% (12/60)\u001b[K\rremote: Counting objects:  21% (13/60)\u001b[K\rremote: Counting objects:  23% (14/60)\u001b[K\rremote: Counting objects:  25% (15/60)\u001b[K\rremote: Counting objects:  26% (16/60)\u001b[K\rremote: Counting objects:  28% (17/60)\u001b[K\rremote: Counting objects:  30% (18/60)\u001b[K\rremote: Counting objects:  31% (19/60)\u001b[K\rremote: Counting objects:  33% (20/60)\u001b[K\rremote: Counting objects:  35% (21/60)\u001b[K\rremote: Counting objects:  36% (22/60)\u001b[K\rremote: Counting objects:  38% (23/60)\u001b[K\rremote: Counting objects:  40% (24/60)\u001b[K\rremote: Counting objects:  41% (25/60)\u001b[K\rremote: Counting objects:  43% (26/60)\u001b[K\rremote: Counting objects:  45% (27/60)\u001b[K\rremote: Counting objects:  46% (28/60)\u001b[K\rremote: Counting objects:  48% (29/60)\u001b[K\rremote: Counting objects:  50% (30/60)\u001b[K\rremote: Counting objects:  51% (31/60)\u001b[K\rremote: Counting objects:  53% (32/60)\u001b[K\rremote: Counting objects:  55% (33/60)\u001b[K\rremote: Counting objects:  56% (34/60)\u001b[K\rremote: Counting objects:  58% (35/60)\u001b[K\rremote: Counting objects:  60% (36/60)\u001b[K\rremote: Counting objects:  61% (37/60)\u001b[K\rremote: Counting objects:  63% (38/60)\u001b[K\rremote: Counting objects:  65% (39/60)\u001b[K\rremote: Counting objects:  66% (40/60)\u001b[K\rremote: Counting objects:  68% (41/60)\u001b[K\rremote: Counting objects:  70% (42/60)\u001b[K\rremote: Counting objects:  71% (43/60)\u001b[K\rremote: Counting objects:  73% (44/60)\u001b[K\rremote: Counting objects:  75% (45/60)\u001b[K\rremote: Counting objects:  76% (46/60)\u001b[K\rremote: Counting objects:  78% (47/60)\u001b[K\rremote: Counting objects:  80% (48/60)\u001b[K\rremote: Counting objects:  81% (49/60)\u001b[K\rremote: Counting objects:  83% (50/60)\u001b[K\rremote: Counting objects:  85% (51/60)\u001b[K\rremote: Counting objects:  86% (52/60)\u001b[K\rremote: Counting objects:  88% (53/60)\u001b[K\rremote: Counting objects:  90% (54/60)\u001b[K\rremote: Counting objects:  91% (55/60)\u001b[K\rremote: Counting objects:  93% (56/60)\u001b[K\rremote: Counting objects:  95% (57/60)\u001b[K\rremote: Counting objects:  96% (58/60)\u001b[K\rremote: Counting objects:  98% (59/60)\u001b[K\rremote: Counting objects: 100% (60/60)\u001b[K\rremote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 60 (delta 18), reused 38 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (60/60), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T36tVKvTwYt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "import re\n",
        "from argparse import Namespace\n",
        "import time\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISqLjmV_kDMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
        "    net.eval()\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "    \n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    return ' '.join(words).encode('utf-8')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Piuuz-ValHdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " y=predict(device, net, \"Sherlock Holmes\".split(), n_vocab, vocab_to_int, int_to_vocab, top_k=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzdwop6Dxrjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Pen :\n",
        "  def __init__(self,flags,brain):\n",
        "    self.Brain=brain\n",
        "    self.Network=self.Brain.Network\n",
        "    self.Data=self.Brain.Data\n",
        "    self.Optimizer=self.Brain.Optimizer\n",
        "    self.flags=flags\n",
        "    \n",
        "    # Temperatur Randomness\n",
        "    def temp_choice(temp):\n",
        "      probs=np.asarray([80-60*temp,60-20*temp,50,40+20*temp,60+20*temp])\n",
        "      return probs/250\n",
        "    \n",
        "    # Writer\n",
        "    def writer(self,words=\"I am\",temperature=0.5,length=70):\n",
        "      self.Network.eval()\n",
        "      state_h, state_c = self.Network.zero_state(1)\n",
        "      state_h = state_h.to(self.flags.device)\n",
        "      state_c = state_c.to(self.flags.device)\n",
        "      \n",
        "      for w in words.split():\n",
        "        ix = torch.tensor([[self.Data.vocab_to_int[w]]]).to(self.flags.device)\n",
        "        output, (state_h, state_c) = self.Network(ix, (state_h, state_c))\n",
        "        top_k=np.random.choice(np.arange(1,6),1,p=temp_choice(temperature))\n",
        "     \n",
        "      _, top_ix = torch.topk(output[0], k=top_k)\n",
        "      choices = top_ix.tolist()\n",
        "      choice = np.random.choice(choices[0])\n",
        "      words.append(self.Data.int_to_vocab[choice])\n",
        "      \n",
        "      for _ in range(length):\n",
        "          ix = torch.tensor([[choice]]).to(self.flags.device)\n",
        "          output, (state_h, state_c) = self.Network(ix, (state_h, state_c))\n",
        "          top_k=np.random.choice(np.arange(1,6),1,p=temp_choice(temperature))\n",
        "          \n",
        "          _, top_ix = torch.topk(output[0], k=top_k)\n",
        "          choices = top_ix.tolist()\n",
        "          choice = np.random.choice(choices[0])\n",
        "          words.append(self.Data.int_to_vocab[choice])\n",
        "\n",
        "      print(' '.join(words))\n",
        "      return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq19D64NV03p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pen=Pen(Pen_flags,br)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mv5MIo7zjTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arthur :\n",
        "  def __init__(self,epochs=50):\n",
        "    self.Data=Data(Data_flags)\n",
        "    self.Network=Network(Brain_flags,Data)\n",
        "    self.Optimizer=Optimizer(self.Network)\n",
        "    self.Brain=Brain(Brain_flags,Optimizer)\n",
        "    self.epochs=epochs\n",
        "    self.Pen=Pen(Pen_flags,Brain)\n",
        "    self.models=[]\n",
        "    self.output=[]\n",
        "    \n",
        "    # Train\n",
        "    def Train(self):\n",
        "      self.Brain.Train(self.epochs)\n",
        "    \n",
        "    # Write\n",
        "    def Write(self):\n",
        "      output.append(self.Pen.Write())\n",
        "      models.append(self.Brain.flags)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrkX8b9p1-IO",
        "colab_type": "code",
        "outputId": "f4be66ec-73ac-4e78-b6fd-d6c16064f0ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "pen.writer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-78ef746255cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Pen' object has no attribute 'writer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-K022YTDnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arthur:\n",
        "  def __init__(self,flags):\n",
        "    self.flags=flags\n",
        "    self.Data=Data(flags)\n",
        "    self.Brain=Brain(flags,self.Data.n_vocab)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKX4DJJdLUHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Class\n",
        "class Data :\n",
        "  def __init__(self,flags):\n",
        "    self.flags=flags\n",
        "    \n",
        "    # Pre Process Data\n",
        "    self.pre_process()\n",
        "    \n",
        "  # Prepare Data For Training\n",
        "  def pre_process(self):\n",
        "    start=time.time()\n",
        "    # Read datat and split to words from files\n",
        "    text=open(self.flags.data_dir).read().split()\n",
        "\n",
        "    # Create Frequency Dictionary\n",
        "    word_counts = Counter(text)\n",
        "    self.sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    # Word2Vec Mapping\n",
        "    self.int_to_vocab = {k: w for k, w in enumerate(self.sorted_vocab)}\n",
        "    self.vocab_to_int = {w: k for k, w in self.int_to_vocab.items()}\n",
        "    self.n_vocab = len(self.int_to_vocab)\n",
        "\n",
        "    # Create Input-Output Data\n",
        "    int_text = [self.vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (self.flags.seq_size * self.flags.batch_size))\n",
        "    in_text = int_text[:num_batches * self.flags.batch_size * self.flags.seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (Data_flags.batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (Data_flags.batch_size, -1))\n",
        "    print(\"Data Preprocessing complete with {} words\".format(self.n_vocab))\n",
        "    self.in_text=in_text\n",
        "    self.out_text=out_text\n",
        "\n",
        "  # Create Input-Output Batch Generator \n",
        "  def get_batches(self):\n",
        "    num_batches = np.prod(self.in_text.shape) // (self.flags.seq_size * self.flags.batch_size)\n",
        "    for i in range(0, num_batches * self.flags.seq_size, self.flags.seq_size):\n",
        "      yield self.in_text[:, i:i+self.flags.seq_size], self.out_text[:, i:i+self.flags.seq_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JKIh3WNwoV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Brain(nn.Module):\n",
        "  def __init__(self,flags,n_vocab):\n",
        "    super(Brain,self).__init__()\n",
        "    self.flags=flags\n",
        "    self.embedding= nn.Embedding(n_vocab, flags.embedding_size)\n",
        "    self.lstm=nn.LSTM(flags.embedding_size, flags.lstm_size, batch_first=True, num_layers=flags.num_layers, dropout=flags.dropout)\n",
        "    self.dense = nn.Linear(flags.lstm_size, n_vocab)\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    self.optimizer = torch.optim.Adam(self.parameters(), lr=self.flags.lr)\n",
        "    self.loss_value=0\n",
        "    self.to(flags.device)\n",
        "    \n",
        "  # Forward Pass\n",
        "  def forward(self, x, prev_state):\n",
        "      embed = self.embedding(x)\n",
        "      output, state = self.lstm(embed, prev_state)\n",
        "      logits = self.dense(output)\n",
        "      return logits, state\n",
        "\n",
        "  # ZeroState Init\n",
        "  def zero_state(self,batch_size):\n",
        "        return (torch.zeros(self.flags.num_layers*(2 if self.flags.is_bidirectional else 1), batch_size, self.flags.lstm_size),\n",
        "                torch.zeros(self.flags.num_layers*(2 if self.flags.is_bidirectional else 1), batch_size, self.flags.lstm_size))\n",
        "  \n",
        "  #Train Function\n",
        "  def Train(self,epochs,Data):\n",
        "    self.train()\n",
        "    for epoch in range(epochs):\n",
        "      start=time.time()\n",
        "      batches=Data.get_batches()\n",
        "      state_h, state_c = self.zero_state(self.flags.lstm_size)\n",
        "      \n",
        "      \n",
        "      # Transfer data to GPU\n",
        "      state_h = state_h.to(self.flags.device)\n",
        "      state_c = state_c.to(self.flags.device)\n",
        "      \n",
        "      for x, y in tqdm(batches):\n",
        "        \n",
        "        # Reset all gradients\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Transfer data to GPU\n",
        "        x = torch.tensor(x).to(self.flags.device)\n",
        "        y = torch.tensor(y).to(self.flags.device)\n",
        "\n",
        "        logits, (state_h, state_c) = self(x, (state_h, state_c))\n",
        "        self.loss = self.criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "        state_h = state_h.detach()\n",
        "        state_c = state_c.detach()\n",
        "\n",
        "        self.loss_value = self.loss.item()\n",
        "        \n",
        "        # Update the network's parameters\n",
        "        self.optimizer.step()\n",
        "        self.loss.backward()\n",
        "        _ = torch.nn.utils.clip_grad_norm_(self.parameters(), self.flags.gradients_norm)\n",
        "        #self.Optimizer.decay(self.loss)\n",
        "        \n",
        "        self.optimizer.step()\n",
        "      print('Epoch: {}/{}'.format(epoch, epochs),'Loss: {}'.format(self.loss_value),'Time Taken : {}'.format(time.time()-start))\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DicE_NN532BN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "3e5aeebf-d487-4e28-f4db-95309dc8f58c"
      },
      "source": [
        "art=Arthur(flags)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Preprocessing complete with 21252 words\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0SV7SVMBLOb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "1a25c32b-93f0-45b4-a534-483bb0dda1bf"
      },
      "source": [
        "art.Brain.Train(2,art.Data)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7it [00:02,  2.54it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-5fb4ef2ca8d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-61-e8927cbf7b6c>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(self, epochs, Data)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Update the network's parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMi1tp3NBP1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "f452fead-082f-4a1c-bfda-675ea7effcf6"
      },
      "source": [
        ""
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46it [00:18,  2.44it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/2 Loss: 5.393995761871338 Time Taken : 18.442371129989624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "46it [00:18,  2.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/2 Loss: 4.833022117614746 Time Taken : 18.931352853775024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-2SNTBBBhPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVv8yswFC1wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}