{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sherlocked.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/someshsingh22/Sherlocked/blob/master/Sherlocked.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTVePy0RwXRE",
        "colab_type": "code",
        "outputId": "6e07b7f0-0cae-45eb-fd3a-7c4be2ca1153",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "! git clone https://github.com/someshsingh22/Sherlocked"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Sherlocked'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/54)\u001b[K\rremote: Counting objects:   3% (2/54)\u001b[K\rremote: Counting objects:   5% (3/54)\u001b[K\rremote: Counting objects:   7% (4/54)\u001b[K\rremote: Counting objects:   9% (5/54)\u001b[K\rremote: Counting objects:  11% (6/54)\u001b[K\rremote: Counting objects:  12% (7/54)\u001b[K\rremote: Counting objects:  14% (8/54)\u001b[K\rremote: Counting objects:  16% (9/54)\u001b[K\rremote: Counting objects:  18% (10/54)\u001b[K\rremote: Counting objects:  20% (11/54)\u001b[K\rremote: Counting objects:  22% (12/54)\u001b[K\rremote: Counting objects:  24% (13/54)\u001b[K\rremote: Counting objects:  25% (14/54)\u001b[K\rremote: Counting objects:  27% (15/54)\u001b[K\rremote: Counting objects:  29% (16/54)\u001b[K\rremote: Counting objects:  31% (17/54)\u001b[K\rremote: Counting objects:  33% (18/54)\u001b[K\rremote: Counting objects:  35% (19/54)\u001b[K\rremote: Counting objects:  37% (20/54)\u001b[K\rremote: Counting objects:  38% (21/54)\u001b[K\rremote: Counting objects:  40% (22/54)\u001b[K\rremote: Counting objects:  42% (23/54)\u001b[K\rremote: Counting objects:  44% (24/54)\u001b[K\rremote: Counting objects:  46% (25/54)\u001b[K\rremote: Counting objects:  48% (26/54)\u001b[K\rremote: Counting objects:  50% (27/54)\u001b[K\rremote: Counting objects:  51% (28/54)\u001b[K\rremote: Counting objects:  53% (29/54)\u001b[K\rremote: Counting objects:  55% (30/54)\u001b[K\rremote: Counting objects:  57% (31/54)\u001b[K\rremote: Counting objects:  59% (32/54)\u001b[K\rremote: Counting objects:  61% (33/54)\u001b[K\rremote: Counting objects:  62% (34/54)\u001b[K\rremote: Counting objects:  64% (35/54)\u001b[K\rremote: Counting objects:  66% (36/54)\u001b[K\rremote: Counting objects:  68% (37/54)\u001b[K\rremote: Counting objects:  70% (38/54)\u001b[K\rremote: Counting objects:  72% (39/54)\u001b[K\rremote: Counting objects:  74% (40/54)\u001b[K\rremote: Counting objects:  75% (41/54)\u001b[K\rremote: Counting objects:  77% (42/54)\u001b[K\rremote: Counting objects:  79% (43/54)\u001b[K\rremote: Counting objects:  81% (44/54)\u001b[K\rremote: Counting objects:  83% (45/54)\u001b[K\rremote: Counting objects:  85% (46/54)\u001b[K\rremote: Counting objects:  87% (47/54)\u001b[K\rremote: Counting objects:  88% (48/54)\u001b[K\rremote: Counting objects:  90% (49/54)\u001b[K\rremote: Counting objects:  92% (50/54)\u001b[K\rremote: Counting objects:  94% (51/54)\u001b[K\rremote: Counting objects:  96% (52/54)\u001b[K\rremote: Counting objects:  98% (53/54)\u001b[K\rremote: Counting objects: 100% (54/54)\u001b[K\rremote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 54 (delta 14), reused 38 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (54/54), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T36tVKvTwYt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "import re\n",
        "from argparse import Namespace\n",
        "import time\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0kRFHqfwdov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISqLjmV_kDMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
        "    net.eval()\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "    \n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    return ' '.join(words).encode('utf-8')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Piuuz-ValHdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " y=predict(device, net, \"Sherlock Holmes\".split(), n_vocab, vocab_to_int, int_to_vocab, top_k=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeFBGSaFlaio",
        "colab_type": "code",
        "outputId": "c795663f-0435-4e74-b2fb-8502effc3b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Sherlock Holmes had said to my own to do with the first thing that I had seen a more powerfully built young man , and the same necessity was growing in the room , and I was to be able for a few minutes , but he had been a very busy man . He was a very tall , dark man with the broad and ready face . He was a perfect savage , tall , strong , strong object was a familiar man , and he was a very tall and fair haired woman hobbled to the table . He was'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMxUXduLwiHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path=\"./Sherlocked/Dataset/Clean/{}.txt\"\n",
        "\n",
        "Data_flags = Namespace(\n",
        "    data_dir=path.format(\"cano\"),\n",
        "    seq_size=64,\n",
        "    batch_size=256,\n",
        ")\n",
        "Brain_flags=Namespace(\n",
        "    num_layers=1,\n",
        "    is_bidirectional=False,\n",
        "    seq_size=Data_flags.seq_size,\n",
        "    batch_size=Data_flags.batch_size,\n",
        "    embedding_size=256,\n",
        "    lstm_size=1024,\n",
        "    gradients_norm=5,\n",
        "    dropout=0.1\n",
        ")\n",
        "Pen_flags=Namespace(\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tKl0vEBospF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Class\n",
        "class Data :\n",
        "  def __init__(self,Data_flags):\n",
        "    self.flags=Data_flags\n",
        "    \n",
        "    # Pre Process Data\n",
        "    self.pre_process()\n",
        "    \n",
        "  # Prepare Data For Training\n",
        "  def pre_process(self):\n",
        "    start=time.time()\n",
        "    # Read datat and split to words from files\n",
        "    text=open(Data_flags.data_dir).read().split()\n",
        "\n",
        "    # Create Frequency Dictionary\n",
        "    word_counts = Counter(text)\n",
        "    self.sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    # Word2Vec Mapping\n",
        "    self.int_to_vocab = {k: w for k, w in enumerate(self.sorted_vocab)}\n",
        "    self.vocab_to_int = {w: k for k, w in self.int_to_vocab.items()}\n",
        "    self.n_vocab = len(self.int_to_vocab)\n",
        "\n",
        "    # Create Input-Output Data\n",
        "    self.int_text = [self.vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(self.int_text) / (Data_flags.seq_size * Data_flags.batch_size))\n",
        "    self.in_text = self.int_text[:num_batches * Data_flags.batch_size * Data_flags.seq_size]\n",
        "    self.out_text = np.zeros_like(self.in_text)\n",
        "    self.out_text[:-1] = self.in_text[1:]\n",
        "    self.out_text[-1] = self.in_text[0]\n",
        "    self.in_text = np.reshape(self.in_text, (Data_flags.batch_size, -1))\n",
        "    self.out_text = np.reshape(self.out_text, (Data_flags.batch_size, -1))\n",
        "    print(\"Data Preprocessing complete with {} words\".format(self.n_vocab))\n",
        "\n",
        "  # Create Input-Output Batch Generator \n",
        "  def get_batches(self):\n",
        "    num_batches = np.prod(self.in_text.shape) // (self.flags.seq_size * self.flags.batch_size)\n",
        "    for i in range(0, num_batches * self.flags.seq_size, self.flags.seq_size):\n",
        "      yield self.in_text[:, i:i+self.flags.seq_size], self.out_text[:, i:i+self.flags.seq_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXJa902jygIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neural Network\n",
        "class Network(nn.Module):\n",
        "  def __init__(self,flags,data):\n",
        "    super(Network, self).__init__()\n",
        "    self.flags=flags\n",
        "    self.data=data\n",
        "\n",
        "    # Embedding Layer , LSTM Stack and Output Layer\n",
        "    self.embedding= nn.Embedding(data.n_vocab, self.flags.embedding_size)\n",
        "    self.lstm=nn.LSTM(self.flags.embedding_size,self.flags.lstm_size,batch_first=True,num_layers=self.flags.num_layers,dropout=self.flags.dropout)\n",
        "    self.dense = nn.Linear(self.flags.lstm_size, data.n_vocab)\n",
        "\n",
        "  # Forward Pass\n",
        "  def forward(self, x, prev_state):\n",
        "      embed = self.embedding(x)\n",
        "      output, state = self.lstm(embed, prev_state)\n",
        "      logits = self.dense(output)\n",
        "      return logits, state\n",
        "\n",
        "  # ZeroState Init\n",
        "  def zero_state(self):\n",
        "        return (torch.zeros(self.flags.num_layers*(2 if self.flags.is_bidirectional else 1), self.flags.batch_size, self.flags.lstm_size),\n",
        "                torch.zeros(self.flags.num_layers*(2 if self.flags.is_bidirectional else 1), self.flags.batch_size, self.flags.lstm_size))\n",
        "\n",
        "# Solver / Optimizer\n",
        "class Optimizer :\n",
        "  def __init__(self,Network):\n",
        "    self.lr=0.02\n",
        "    self.Network=Network\n",
        "\n",
        "    # Defining Loss function and Optimizer\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    self.optimizer = torch.optim.Adam(self.Network.parameters(), lr=self.lr)\n",
        "\n",
        "  def decay(self,loss):\n",
        "\n",
        "    # Update with Factor or value\n",
        "    def factor_update(factor):\n",
        "      for group in self.optimizer.param_groups:\n",
        "        group['lr']*=value\n",
        "      self.lr*=value\n",
        "    def value_update(value):\n",
        "      for group in self.optimizer.param_groups:\n",
        "        group['lr']=value\n",
        "      self.lr=value\n",
        "\n",
        "    if self.lr > 0.001:\n",
        "      # Initial Fast Changes\n",
        "      if loss > 10 :\n",
        "        self.value_update(0.02)\n",
        "      elif loss > 7 :\n",
        "        self.value_update(0.01)\n",
        "      #slower changes\n",
        "      elif loss > 4 :\n",
        "        self.value_update(0.005)\n",
        "      #exponential drop\n",
        "      else :\n",
        "        self.factor_update(0.97)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wZbyrqlys6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neural Network aka Brain\n",
        "class Brain :\n",
        "  def __init__(self,flags,Optimizer):\n",
        "    self.device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.flags=flags\n",
        "    self.Optimizer=Optimizer\n",
        "    self.Network=self.Optimizer.Network\n",
        "    self.data=self.Network.data\n",
        "    self.Network.to(self.device)\n",
        "    self.loss_value=0\n",
        "    \n",
        "  #Train Function\n",
        "  def Train(self,epochs):\n",
        "    self.Network.train()\n",
        "    for epoch in range(epochs):\n",
        "      start=time.time()\n",
        "      batches=self.data.get_batches()\n",
        "      state_h, state_c = self.Network.zero_state()\n",
        "      \n",
        "      \n",
        "      # Transfer data to GPU\n",
        "      state_h = state_h.to(self.device)\n",
        "      state_c = state_c.to(self.device)\n",
        "      \n",
        "      for x, y in tqdm(batches):\n",
        "        \n",
        "        # Reset all gradients\n",
        "        self.Optimizer.optimizer.zero_grad()\n",
        "\n",
        "        # Transfer data to GPU\n",
        "        x = torch.tensor(x).to(self.device)\n",
        "        y = torch.tensor(y).to(self.device)\n",
        "\n",
        "        logits, (state_h, state_c) = self.Network(x, (state_h, state_c))\n",
        "        self.loss = self.Optimizer.criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "        state_h = state_h.detach()\n",
        "        state_c = state_c.detach()\n",
        "\n",
        "        self.loss_value = self.loss.item()\n",
        "        \n",
        "        # Update the network's parameters\n",
        "        self.Optimizer.optimizer.step()\n",
        "        self.loss.backward()\n",
        "        _ = torch.nn.utils.clip_grad_norm_(self.Network.parameters(), self.flags.gradients_norm)\n",
        "        #self.Optimizer.decay(self.loss)\n",
        "        \n",
        "        self.Optimizer.optimizer.step()\n",
        "      print('Epoch: {}/{}'.format(epoch, epochs),'Loss: {}'.format(self.loss_value),'Time Taken : {}'.format(time.time()-start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygDtVW1yqTdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "d=Data(Data_flags)\n",
        "n=Network(Brain_flags,d)\n",
        "o=Optimizer(n)\n",
        "br=Brain(Brain_flags,o)\n",
        "br.Train(2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzdwop6Dxrjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Pen :\n",
        "  def __init__()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mv5MIo7zjTu",
        "colab_type": "code",
        "outputId": "38adb42c-204a-4d3f-8d0c-841960e9c15a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11it [00:08,  1.28it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-129902cafd9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBrain_flags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-443cdb74558f>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Update the network's parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrkX8b9p1-IO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9e004f12-653d-4f61-e917-96827a7967be"
      },
      "source": [
        "print(data)"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.Data object at 0x7f65a191b9e8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-K022YTDnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}