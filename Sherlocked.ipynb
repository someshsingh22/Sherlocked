{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sherlocked.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/someshsingh22/Sherlocked/blob/master/Sherlocked.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTVePy0RwXRE",
        "colab_type": "code",
        "outputId": "d7806b4f-e06f-442e-c108-9d2f99453396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "! git clone https://github.com/someshsingh22/Sherlocked"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Sherlocked'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 51 (delta 13), reused 39 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (51/51), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T36tVKvTwYt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "import re\n",
        "from argparse import Namespace\n",
        "import time\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0kRFHqfwdov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir=\"./Sherlocked/Dataset/Clean/{}.txt\"\n",
        "flags = Namespace(\n",
        "    train_file=data_dir.format(\"cano\"),\n",
        "    seq_size=64,\n",
        "    batch_size=256,\n",
        "    embedding_size=256,\n",
        "    lstm_size=1024,\n",
        "    gradients_norm=5,\n",
        "    initial_words=['I', 'am'],\n",
        "    predict_top_k=5,\n",
        "    checkpoint_path='./checkpoint',)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m0wKU5cxunU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "    with open(train_file, 'r') as f:\n",
        "        text = f.read()\n",
        "    text = text.split()\n",
        "\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0by4BPp1MYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx7pE39a7JDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size, lstm_layers=2, is_bidirectional=False):\n",
        "        super(RNNModule, self).__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm_layers=lstm_layers\n",
        "        self.is_bidirectional=is_bidirectional\n",
        "        self.lstm = nn.LSTM(embedding_size,lstm_size,batch_first=True,num_layers=lstm_layers,dropout=0.1,bidirectional=is_bidirectional)\n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "    \n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "    \n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(lstm_layers*(2 if is_bidirectional else 1), batch_size, self.lstm_size),\n",
        "                torch.zeros(self.lstm_layers*(2 if is_bidirectional else 1), batch_size, self.lstm_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQoNfz647fFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss_and_train_op(net, lr=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    return criterion, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtkVHe613vA_",
        "colab_type": "code",
        "outputId": "c56b0a70-83f7-468b-cd3e-c28b34a600b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "def lr_update(optimizer,decay):\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-101-f8c589f0f750>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQp_GP_awrOr",
        "colab_type": "code",
        "outputId": "7ad38633-2930-49af-dd93-4c77267c94b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(flags.train_file, flags.batch_size, flags.seq_size)\n",
        "net = RNNModule(n_vocab, flags.seq_size,flags.embedding_size, flags.lstm_size)\n",
        "net = net.to(device)\n",
        "criterion, optimizer = get_loss_and_train_op(net, 0.01)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 21252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9VWmk6stxjz",
        "colab_type": "code",
        "outputId": "4fc01452-1b3a-40a1-f25a-8edeee322e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "in_text.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 2944)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbpKQ8hV7lzI",
        "colab_type": "code",
        "outputId": "aa3f56ac-37fa-424e-82f6-6635443c1b92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        }
      },
      "source": [
        "iteration = 0\n",
        "epochs=50\n",
        "for e in range(epochs):\n",
        "  start=time.time()\n",
        "  batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
        "  state_h, state_c = net.zero_state(flags.batch_size)\n",
        "\n",
        "  # Transfer data to GPU\n",
        "  state_h = state_h.to(device)\n",
        "  state_c = state_c.to(device)\n",
        "  for x, y in batches:\n",
        "    iteration += 1\n",
        "\n",
        "    # Tell it we are in training mode\n",
        "    net.train()\n",
        "\n",
        "    # Reset all gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Transfer data to GPU\n",
        "    x = torch.tensor(x).to(device)\n",
        "    y = torch.tensor(y).to(device)\n",
        "\n",
        "    logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "    loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "    state_h = state_h.detach()\n",
        "    state_c = state_c.detach()\n",
        "\n",
        "    loss_value = loss.item()\n",
        "\n",
        "    # Update the network's parameters\n",
        "    optimizer.step()\n",
        "    loss.backward()\n",
        "\n",
        "    _ = torch.nn.utils.clip_grad_norm_(net.parameters(), flags.gradients_norm)\n",
        "    optimizer.step()\n",
        "  print('Epoch: {}/{}'.format(e, epochs),'Loss: {}'.format(loss_value),'Time Taken : {}'.format(time.time()-start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/50 Loss: 5.400121212005615 Time Taken : 24.643052339553833\n",
            "Epoch: 1/50 Loss: 5.212407112121582 Time Taken : 25.950422763824463\n",
            "Epoch: 2/50 Loss: 5.067686557769775 Time Taken : 25.42293691635132\n",
            "Epoch: 3/50 Loss: 4.9876298904418945 Time Taken : 25.024222373962402\n",
            "Epoch: 4/50 Loss: 4.886146545410156 Time Taken : 25.23982071876526\n",
            "Epoch: 5/50 Loss: 4.814850807189941 Time Taken : 25.432995557785034\n",
            "Epoch: 6/50 Loss: 4.738186359405518 Time Taken : 25.31384539604187\n",
            "Epoch: 7/50 Loss: 4.681518077850342 Time Taken : 25.288341522216797\n",
            "Epoch: 8/50 Loss: 4.595388412475586 Time Taken : 25.334911823272705\n",
            "Epoch: 9/50 Loss: 4.561583995819092 Time Taken : 25.292315006256104\n",
            "Epoch: 10/50 Loss: 4.519013404846191 Time Taken : 25.317748069763184\n",
            "Epoch: 11/50 Loss: 4.483057498931885 Time Taken : 25.31722855567932\n",
            "Epoch: 12/50 Loss: 4.442838668823242 Time Taken : 25.300121784210205\n",
            "Epoch: 13/50 Loss: 4.407785892486572 Time Taken : 25.300395250320435\n",
            "Epoch: 14/50 Loss: 4.3404927253723145 Time Taken : 25.29112672805786\n",
            "Epoch: 15/50 Loss: 4.290738582611084 Time Taken : 25.31314730644226\n",
            "Epoch: 16/50 Loss: 4.271694660186768 Time Taken : 25.324208974838257\n",
            "Epoch: 17/50 Loss: 4.22514009475708 Time Taken : 25.30818462371826\n",
            "Epoch: 18/50 Loss: 4.196774482727051 Time Taken : 25.313586235046387\n",
            "Epoch: 19/50 Loss: 4.165204048156738 Time Taken : 25.341542959213257\n",
            "Epoch: 20/50 Loss: 4.12469482421875 Time Taken : 25.319770574569702\n",
            "Epoch: 21/50 Loss: 4.0897111892700195 Time Taken : 25.326916217803955\n",
            "Epoch: 22/50 Loss: 4.066350936889648 Time Taken : 25.31497025489807\n",
            "Epoch: 23/50 Loss: 4.033892631530762 Time Taken : 25.303608417510986\n",
            "Epoch: 24/50 Loss: 4.014594554901123 Time Taken : 25.33030080795288\n",
            "Epoch: 25/50 Loss: 3.973609447479248 Time Taken : 25.298649311065674\n",
            "Epoch: 26/50 Loss: 3.94643235206604 Time Taken : 25.31278395652771\n",
            "Epoch: 27/50 Loss: 3.925605058670044 Time Taken : 25.32035756111145\n",
            "Epoch: 28/50 Loss: 3.9134442806243896 Time Taken : 25.3095703125\n",
            "Epoch: 29/50 Loss: 3.882599353790283 Time Taken : 25.322471618652344\n",
            "Epoch: 30/50 Loss: 3.838639497756958 Time Taken : 25.30912685394287\n",
            "Epoch: 31/50 Loss: 3.8146374225616455 Time Taken : 25.301326274871826\n",
            "Epoch: 32/50 Loss: 3.7987303733825684 Time Taken : 25.32478642463684\n",
            "Epoch: 33/50 Loss: 3.763123035430908 Time Taken : 25.301098585128784\n",
            "Epoch: 34/50 Loss: 3.7467312812805176 Time Taken : 25.300480127334595\n",
            "Epoch: 35/50 Loss: 3.715843439102173 Time Taken : 25.326170206069946\n",
            "Epoch: 36/50 Loss: 3.6936299800872803 Time Taken : 25.304205417633057\n",
            "Epoch: 37/50 Loss: 3.7001492977142334 Time Taken : 25.324041604995728\n",
            "Epoch: 38/50 Loss: 3.6645689010620117 Time Taken : 25.312556743621826\n",
            "Epoch: 39/50 Loss: 3.649240732192993 Time Taken : 25.29323387145996\n",
            "Epoch: 40/50 Loss: 3.6478395462036133 Time Taken : 25.33007788658142\n",
            "Epoch: 41/50 Loss: 3.61149263381958 Time Taken : 25.294870615005493\n",
            "Epoch: 42/50 Loss: 3.5877835750579834 Time Taken : 25.30774211883545\n",
            "Epoch: 43/50 Loss: 3.600670099258423 Time Taken : 25.31431007385254\n",
            "Epoch: 44/50 Loss: 3.578190326690674 Time Taken : 25.303166389465332\n",
            "Epoch: 45/50 Loss: 3.5544357299804688 Time Taken : 25.305322408676147\n",
            "Epoch: 46/50 Loss: 3.5379555225372314 Time Taken : 25.304874658584595\n",
            "Epoch: 47/50 Loss: 3.5327048301696777 Time Taken : 25.29872989654541\n",
            "Epoch: 48/50 Loss: 3.51456356048584 Time Taken : 25.3118679523468\n",
            "Epoch: 49/50 Loss: 3.4877591133117676 Time Taken : 25.305906295776367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISqLjmV_kDMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
        "    net.eval()\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "    \n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    return ' '.join(words).encode('utf-8')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Piuuz-ValHdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " y=predict(device, net, \"Sherlock Holmes\".split(), n_vocab, vocab_to_int, int_to_vocab, top_k=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeFBGSaFlaio",
        "colab_type": "code",
        "outputId": "c795663f-0435-4e74-b2fb-8502effc3b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Sherlock Holmes had said to my own to do with the first thing that I had seen a more powerfully built young man , and the same necessity was growing in the room , and I was to be able for a few minutes , but he had been a very busy man . He was a very tall , dark man with the broad and ready face . He was a perfect savage , tall , strong , strong object was a familiar man , and he was a very tall and fair haired woman hobbled to the table . He was'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMxUXduLwiHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path=\"./Sherlocked/Dataset/Clean/{}.txt\"\n",
        "\n",
        "Data_flags = Namespace(\n",
        "    data_dir=path.format(\"cano\"),\n",
        "    seq_size=64,\n",
        "    batch_size=256,\n",
        ")\n",
        "Brain_flags=Namespace(\n",
        "    num_layers=1,\n",
        "    is_bidirectional=False,\n",
        "    seq_size=Data_flags.seq_size,\n",
        "    batch_size=Data_flags.batch_size,\n",
        "    embedding_size=256,\n",
        "    lstm_size=256,\n",
        "    gradients_norm=5,\n",
        "    dropout=0.1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tKl0vEBospF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Class\n",
        "class Data :\n",
        "  def __init__(self,Data_flags):\n",
        "    self.flags=Data_flags\n",
        "    \n",
        "    # Pre Process Data\n",
        "    self.pre_process()\n",
        "    \n",
        "  # Prepare Data For Training\n",
        "  def pre_process(self):\n",
        "    start=time.time()\n",
        "    # Read datat and split to words from files\n",
        "    text=open(Data_flags.data_dir).read().split()\n",
        "\n",
        "    # Create Frequency Dictionary\n",
        "    word_counts = Counter(text)\n",
        "    self.sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    # Word2Vec Mapping\n",
        "    self.int_to_vocab = {k: w for k, w in enumerate(self.sorted_vocab)}\n",
        "    self.vocab_to_int = {w: k for k, w in self.int_to_vocab.items()}\n",
        "    self.n_vocab = len(self.int_to_vocab)\n",
        "\n",
        "    # Create Input-Output Data\n",
        "    self.int_text = [self.vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(self.int_text) / (Data_flags.seq_size * Data_flags.batch_size))\n",
        "    self.in_text = self.int_text[:num_batches * Data_flags.batch_size * Data_flags.seq_size]\n",
        "    self.out_text = np.zeros_like(self.in_text)\n",
        "    self.out_text[:-1] = self.in_text[1:]\n",
        "    self.out_text[-1] = self.in_text[0]\n",
        "    self.in_text = np.reshape(self.in_text, (Data_flags.batch_size, -1))\n",
        "    self.out_text = np.reshape(self.out_text, (Data_flags.batch_size, -1))\n",
        "    print(\"Data Preprocessing complete with {} words\".format(self.n_vocab))\n",
        "\n",
        "  # Create Input-Output Batch Generator \n",
        "  def get_batches(self):\n",
        "    num_batches = np.prod(self.in_text.shape) // (self.flags.seq_size * self.flags.batch_size)\n",
        "    for i in range(0, num_batches * self.flags.seq_size, self.flags.seq_size):\n",
        "      yield self.in_text[:, i:i+self.flags.seq_size], self.out_text[:, i:i+self.flags.seq_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXJa902jygIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neural Network\n",
        "class Network(nn.Module):\n",
        "  def __init__(self,flags,data):\n",
        "    super(Network, self).__init__()\n",
        "    self.flags=flags\n",
        "\n",
        "    # Embedding Layer , LSTM Stack and Output Layer\n",
        "    self.embedding= nn.Embedding(data.n_vocab, 256)\n",
        "    self.lstm=nn.LSTM(256,256,batch_first=True,num_layers=2,dropout=0.1)\n",
        "    self.dense = nn.Linear(256, data.n_vocab)\n",
        "\n",
        "  # Forward Pass\n",
        "  def forward(self, x, prev_state):\n",
        "      embed = self.embedding(x)\n",
        "      output, state = self.lstm(embed, prev_state)\n",
        "      logits = self.dense(output)\n",
        "      return logits, state\n",
        "\n",
        "  # ZeroState Init\n",
        "  def zero_state(self):\n",
        "        return (torch.zeros(self.flags.num_layers*(2 if self.flags.is_bidirectional else 1), self.flags.batch_size, self.flags.lstm_size),\n",
        "                torch.zeros(self.flags.num_layers*(2 if self.flags.is_bidirectional else 1), self.flags.batch_size, self.flags.lstm_size))\n",
        "\n",
        "# Solver / Optimizer\n",
        "class Optimizer :\n",
        "  def __init__(self,Network):\n",
        "    self.lr=0.02\n",
        "    self.Network=Network\n",
        "\n",
        "    # Defining Loss function and Optimizer\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    self.optimizer = torch.optim.Adam(self.Network.parameters(), lr=self.lr)\n",
        "\n",
        "  def decay(self,loss):\n",
        "\n",
        "    # Update with Factor or value\n",
        "    def factor_update(self,factor):\n",
        "      for group in self.optimizer.param_groups:\n",
        "        group['lr']*=value\n",
        "      self.lr*=value\n",
        "    def value_update(self,value):\n",
        "      for group in self.optimizer.param_groups:\n",
        "        group['lr']=value\n",
        "      self.lr=value\n",
        "\n",
        "    if self.lr > 0.001:\n",
        "      # Initial Fast Changes\n",
        "      if loss > 10 :\n",
        "        value_update(0.02)\n",
        "      elif loss > 7 :\n",
        "        value_update(0.01)\n",
        "      #slower changes\n",
        "      elif loss > 4 :\n",
        "        value_update(0.005)\n",
        "      #exponential drop\n",
        "      else :\n",
        "        factor_update(0.97)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygDtVW1yqTdq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "447c56da-54c7-4f41-fa95-f128a502f0a7"
      },
      "source": [
        "d=Data(Data_flags)\n",
        "n=Network(Brain_flags,d)\n",
        "o=Optimizer(n)"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Preprocessing complete with 21252 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wZbyrqlys6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neural Network aka Brain\n",
        "class Brain :\n",
        "  def __init__(self,flags,data,Network):\n",
        "    self.data=data\n",
        "    self.device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.loss=0\n",
        "    self.flags=flags\n",
        "    self.Network=Network\n",
        "    self.Optimizer=Optimizer(self.Network)\n",
        "    self.Network.to(self.device)\n",
        "    \n",
        "  #Train Function\n",
        "  def Train(self,epochs):\n",
        "    self.Network.train()\n",
        "    for epoch in range(epochs):\n",
        "      start=time.time()\n",
        "      batches=self.data.get_batches()\n",
        "      state_h, state_c = self.Network.zero_state()\n",
        "      \n",
        "      \n",
        "      # Transfer data to GPU\n",
        "      state_h = state_h.to(self.device)\n",
        "      state_c = state_c.to(self.device)\n",
        "      \n",
        "      for x, y in tqdm(batches):\n",
        "        \n",
        "        # Reset all gradients\n",
        "        self.Optimizer.optimizer.zero_grad()\n",
        "\n",
        "        # Transfer data to GPU\n",
        "        x = torch.tensor(x).to(self.device)\n",
        "        y = torch.tensor(y).to(self.device)\n",
        "        \n",
        "        logits, (state_h, state_c) = self.Network(x, (state_h, state_c))\n",
        "        self.loss = self.Optimizer.criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "        state_h = state_h.detach()\n",
        "        state_c = state_c.detach()\n",
        "\n",
        "        self.loss = loss.item()\n",
        "        \n",
        "        # Update the network's parameters\n",
        "        self.Optimizeroptimizer.step()\n",
        "        self.loss.backward()\n",
        "        _ = torch.nn.utils.clip_grad_norm_(self.Network.parameters(), self.flags.gradients_norm)\n",
        "        self.Optimizer.decay(self.loss)\n",
        "        \n",
        "        self.Optimizer.optimizer.step()\n",
        "      print('Epoch: {}/{}'.format(epoch, epochs),'Loss: {}'.format(self.loss),'Time Taken : {}'.format(time.time()-start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzdwop6Dxrjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummary import summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mv5MIo7zjTu",
        "colab_type": "code",
        "outputId": "3b65e78a-ecb9-406a-f529-09f030bd2b08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "br=Brain(Brain_flags,d,n)\n",
        "br.Train(2)"
      ],
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-317-129902cafd9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBrain_flags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-316-5af20683be3a>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'prev_state'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrkX8b9p1-IO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9e004f12-653d-4f61-e917-96827a7967be"
      },
      "source": [
        "print(data)"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.Data object at 0x7f65a191b9e8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-K022YTDnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}